{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ci4tixx087",
   "source": "# Step 2: Dimensionality Reduction on Gene Expression Counts\n\n## What is this step doing?\n\nWe have a dataset where each data point (a tissue \"spot\") is described by **33,538 features** — the raw integer count of how many mRNA molecules of each gene were detected at that location. Our goal downstream is to train a small neural network, so we need a compact, informative representation of each spot first.\n\nThis is a standard **unsupervised dimensionality reduction** problem:\n\n$$\\mathbf{X} \\in \\mathbb{N}^{N \\times G} \\xrightarrow{\\text{dim. reduction}} \\mathbf{A} \\in \\mathbb{R}^{N \\times K}$$\n\nwith $N \\approx 4{,}000$ spots, $G = 33{,}538$ genes, $K = 14$ latent dimensions.\n\nThe output matrix $\\mathbf{A}$ becomes the **reconstruction target** for the GASTON autoencoder in the next step: the decoder must reproduce $\\mathbf{A}_i$ from a single scalar coordinate per spot.\n\n---\n\n## Several methods could work here — this is an open design choice\n\nThis step is not unique to spatial transcriptomics. Any method that produces a fixed-size embedding from a high-dimensional feature vector is a candidate. Here are the main options:\n\n| Method | Core idea | Notes for this task |\n|--------|-----------|---------------------|\n| **PCA** | SVD on the data matrix; find $K$ directions of max variance | Fast and simple. Works reasonably well after a log-transform of counts (`log(X+1)`). A perfectly valid baseline. |\n| **NMF** | $\\mathbf{X} \\approx \\mathbf{Z}\\mathbf{W}^\\top$ with $\\mathbf{Z}, \\mathbf{W} \\geq 0$ | Non-negativity gives interpretable parts-based factors. Also works after log-transform. Good alternative to try. |\n| **scVI** | VAE with a Zero-Inflated Negative Binomial (ZINB) decoder — learns a stochastic latent code | Most expressive model; handles dropout noise best. Slower to train. Latent space is stochastic (reparameterization trick), so using it as a regression target needs care. Worth trying as an ablation. |\n| **GLM-PCA** *(used here)* | Matrix factorization fitted by Poisson MLE — replaces the Gaussian reconstruction loss with a count-appropriate likelihood | Designed specifically for raw count matrices; no pre-normalization needed. The GASTON paper uses this, so we follow it as our default. |\n\nAll four methods output an $(N, K)$ embedding matrix that can slot directly into the rest of the GASTON pipeline. **Swapping this step is one of the cleanest ablations you can run** — same neural network, different input representation.\n\n---\n\n## GLM-PCA — the model we use\n\nGLM-PCA (Townes et al., *Genome Biology* 2019) replaces the Frobenius loss in PCA with a **Poisson log-likelihood**:\n\n$$X_{ig} \\sim \\text{Poisson}\\!\\left(\\exp\\!\\left(\\mathbf{z}_i^\\top \\mathbf{w}_g + \\mu_g\\right)\\right)$$\n\n- $\\mathbf{z}_i \\in \\mathbb{R}^K$ — latent embedding for spot $i$ (stacked into output matrix **A**)\n- $\\mathbf{w}_g \\in \\mathbb{R}^K$ — loading vector for gene $g$\n- $\\mu_g \\in \\mathbb{R}$ — gene-specific intercept (absorbs baseline expression level)\n\nFitting = **MLE**: maximize $\\sum_{i,g} \\log p(X_{ig} \\mid \\mathbf{z}_i, \\mathbf{w}_g, \\mu_g)$ jointly over all parameters.\nOptimization uses alternating Newton updates — conceptually similar to coordinate descent.\n\nThe log-link $\\exp(\\cdot)$ ensures predicted counts are always non-negative, which is consistent with how count data actually behaves.\n\n---\n\n## Output of this notebook\n\n```\nglmpca.npy    shape (N, K=14)  — embedding A, reconstruction target for GASTON decoder\ncoords_mat.npy  shape (N, 2)   — (x, y) spatial coordinates, input to GASTON encoder\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 2: GLM-PCA — Dimensionality Reduction on Raw Gene Expression Counts\n",
    "# =============================================================================\n",
    "# EECS 545 Project: C-GASTON\n",
    "# Sections processed: 151507, 151508 (Sample 1) | 151673, 151674 (Sample 3)\n",
    "#\n",
    "# WHY THIS STEP EXISTS\n",
    "# --------------------\n",
    "# Our gene expression matrix X has shape (N, G) = (~4000 spots, 33538 genes).\n",
    "# That's 33k-dimensional input — way too high for the GASTON neural network to\n",
    "# train on directly.  We need a compact, informative embedding.\n",
    "#\n",
    "# Why not just use standard PCA?\n",
    "# PCA assumes continuous, roughly Gaussian features.  Gene expression data is\n",
    "# raw integer counts (UMIs) that follow a Poisson or Negative-Binomial\n",
    "# distribution — extremely sparse (>90% zeros) and highly skewed.  Applying\n",
    "# PCA to count data violates its assumptions and gives poor latent factors.\n",
    "#\n",
    "# GLM-PCA (Townes et al., Genome Biology 2019) fixes this by replacing the\n",
    "# Gaussian likelihood in PCA with a Poisson likelihood.  It's essentially\n",
    "# a matrix factorization with a log-linear model:\n",
    "#\n",
    "#   X_{ig} ~ Poisson( exp( z_i^T w_g + mu_g ) )\n",
    "#\n",
    "# where:\n",
    "#   z_i  in R^K  = latent factor for spot i       (what we want)\n",
    "#   w_g  in R^K  = loading vector for gene g\n",
    "#   mu_g in R    = gene-specific intercept (library size offset)\n",
    "#\n",
    "# The parameters {z_i, w_g, mu_g} are found by maximum likelihood — the same\n",
    "# MLE framework we covered in EECS 545.\n",
    "#\n",
    "# OUTPUT\n",
    "# ------\n",
    "# A = [z_1, z_2, ..., z_N]^T   shape: (N, K=14)\n",
    "# This K=14 dimensional embedding is what GASTON's decoder tries to reconstruct\n",
    "# from the scalar isodepth.  It acts as the \"compressed ground truth\" for\n",
    "# the autoencoder reconstruction loss.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from glmpca import glmpca\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Paths\n",
    "# =============================================================================\n",
    "\n",
    "BASE_DIR = '/home/siruilf/A_new_dataset_for_gaston'\n",
    "\n",
    "SAMPLE1_DATA_DIR = f'{BASE_DIR}/ST_Data/DLPFC_sample1/original_data'\n",
    "SAMPLE3_DATA_DIR = f'{BASE_DIR}/ST_Data/DLPFC_sample3/original_data'\n",
    "\n",
    "GLMPCA_OUTPUT_DIR = f'{BASE_DIR}/2.GLM_PC/glmpca_results'\n",
    "os.makedirs(GLMPCA_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"GLM-PCA output directory: {GLMPCA_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Slices to process\n",
    "# =============================================================================\n",
    "# We use 2 sections from each of the two donors:\n",
    "#   Sample 1 (donor 1): 151507, 151508\n",
    "#   Sample 3 (donor 2): 151673, 151674\n",
    "# Each section is processed independently — GLM-PCA is per-slice.\n",
    "\n",
    "slices_to_process = {\n",
    "    '151507': f'{SAMPLE1_DATA_DIR}/151507.h5ad',\n",
    "    '151508': f'{SAMPLE1_DATA_DIR}/151508.h5ad',\n",
    "    '151673': f'{SAMPLE3_DATA_DIR}/151673.h5ad',\n",
    "    '151674': f'{SAMPLE3_DATA_DIR}/151674.h5ad',\n",
    "}\n",
    "\n",
    "print(\"Slices to process:\")\n",
    "for sid, path in slices_to_process.items():\n",
    "    print(f\"  {sid}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GLM-PCA Hyperparameters\n",
    "# =============================================================================\n",
    "#\n",
    "# K = num_dims: number of latent dimensions\n",
    "# -----------------------------------------\n",
    "# Following the GASTON paper and tutorial, we set K = 2 * Q, where Q is the\n",
    "# number of spatial domains.  For DLPFC: Q=7 (layers L1-L6 + white matter),\n",
    "# so K = 14.  The factor of 2 gives the decoder enough capacity to represent\n",
    "# both the mean expression and its spatial gradient per domain.\n",
    "# (Verified from GASTON cerebellum tutorial: num_dims=8 for Q=4 clusters.)\n",
    "#\n",
    "# penalty\n",
    "# -------\n",
    "# L2 regularization on the loadings W.  Prevents overfitting in the MLE\n",
    "# optimization — same role as weight decay in gradient descent (EECS 545).\n",
    "#\n",
    "# num_iters / eps\n",
    "# ---------------\n",
    "# GLM-PCA is optimized by an alternating MLE procedure (similar to EM).\n",
    "# num_iters caps the outer loop; eps is the convergence threshold on\n",
    "# relative change in deviance (= -2 * log-likelihood).\n",
    "#\n",
    "# num_genes\n",
    "# ---------\n",
    "# We feed ALL 33,538 genes to GLM-PCA (num_genes=None).\n",
    "# If you want faster runs, set num_genes=30000 to use only the top 30k genes\n",
    "# ranked by total UMI count — this is the strategy from the GASTON tutorial.\n",
    "# Note: GASTON does NOT use HVG (highly variable gene) selection here.\n",
    "#       Genes are ranked by total expression, not variability.\n",
    "\n",
    "num_dims  = 14      # K = 2 * Q = 2 * 7\n",
    "penalty   = 10      # L2 regularization on loadings W\n",
    "num_iters = 30      # max outer iterations\n",
    "eps       = 1e-4    # convergence threshold on relative deviance change\n",
    "num_genes = None    # None = use all genes; set 30000 for faster runs\n",
    "spot_umi_threshold = 0  # drop spots with total UMI below this (0 = keep all)\n",
    "\n",
    "print(\"GLM-PCA parameters:\")\n",
    "print(f\"  K (latent dims)  : {num_dims}  [ = 2 x 7 DLPFC layers ]\")\n",
    "print(f\"  penalty          : {penalty}\")\n",
    "print(f\"  max iterations   : {num_iters}\")\n",
    "print(f\"  convergence eps  : {eps}\")\n",
    "print(f\"  genes used       : {'all' if num_genes is None else num_genes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Helper functions\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_extract(h5ad_path, spot_umi_threshold=0):\n",
    "    \"\"\"\n",
    "    Load one h5ad file and extract the three arrays GASTON needs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    counts_mat : ndarray (N, G)  raw UMI counts  — the design matrix X\n",
    "    coords_mat : ndarray (N, 2)  (x, y) spatial coordinates\n",
    "    gene_labels: ndarray (G,)   gene names\n",
    "    \"\"\"\n",
    "    adata = sc.read_h5ad(h5ad_path)\n",
    "\n",
    "    # .X may be a scipy sparse matrix; convert to dense numpy array\n",
    "    counts_mat = adata.X.toarray() if hasattr(adata.X, 'toarray') else np.array(adata.X)\n",
    "    coords_mat = adata.obsm['spatial']\n",
    "    gene_labels = np.array(adata.var_names)\n",
    "\n",
    "    # Optional: remove very low-quality spots (total UMI < threshold)\n",
    "    if spot_umi_threshold > 0:\n",
    "        keep = np.sum(counts_mat, axis=1) >= spot_umi_threshold\n",
    "        print(f\"  Dropping {(~keep).sum()} spots below UMI threshold {spot_umi_threshold}\")\n",
    "        counts_mat = counts_mat[keep]\n",
    "        coords_mat = coords_mat[keep]\n",
    "\n",
    "    return counts_mat, coords_mat, gene_labels\n",
    "\n",
    "\n",
    "def run_glmpca(counts_mat, num_dims, penalty, num_iters, eps, num_genes=None):\n",
    "    \"\"\"\n",
    "    Fit GLM-PCA on one slice's count matrix.\n",
    "\n",
    "    The model optimized here is:\n",
    "        X_{ig} ~ Poisson( exp( z_i^T w_g + mu_g ) )\n",
    "    Parameters {z_i, w_g, mu_g} are estimated by MLE.\n",
    "    Optimization is an alternating least-squares / Newton procedure\n",
    "    that minimizes the Poisson deviance (= -2 * log-likelihood).\n",
    "\n",
    "    The glmpca library expects shape (G, N) — genes as rows — so we transpose.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : ndarray (N, K)   latent factor matrix  [ = z_i stacked as rows ]\n",
    "    \"\"\"\n",
    "    # Gene selection: top genes by total UMI count (not HVG)\n",
    "    if num_genes is None or num_genes >= counts_mat.shape[1]:\n",
    "        X = counts_mat\n",
    "        print(f\"  Using all {X.shape[1]} genes\")\n",
    "    else:\n",
    "        top_idx = np.argsort(np.sum(counts_mat, axis=0))[-num_genes:]\n",
    "        X = counts_mat[:, top_idx]\n",
    "        print(f\"  Using top {X.shape[1]} genes by total UMI\")\n",
    "\n",
    "    # GLM-PCA call — matches GASTON tutorial exactly\n",
    "    # Input shape: (G, N)  (library convention)\n",
    "    res = glmpca.glmpca(\n",
    "        X.T,\n",
    "        num_dims,\n",
    "        fam=\"poi\",\n",
    "        penalty=penalty,\n",
    "        verbose=True,\n",
    "        ctl={\"maxIter\": num_iters, \"eps\": eps, \"optimizeTheta\": True}\n",
    "    )\n",
    "\n",
    "    A = res['factors']   # shape (N, K)\n",
    "    return A\n",
    "\n",
    "\n",
    "print(\"Functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run GLM-PCA on each slice and save outputs\n",
    "# =============================================================================\n",
    "# Runtime note: each slice takes roughly 10–30 min depending on hardware.\n",
    "# The deviance printed each iteration should decrease monotonically.\n",
    "# Convergence is reached when relative change < eps.\n",
    "#\n",
    "# What we save\n",
    "# ------------\n",
    "# counts_mat.npy  : raw X   (N, G)   — needed if you want to re-run GLM-PCA\n",
    "# coords_mat.npy  : (x,y)   (N, 2)   — spatial inputs to GASTON encoder\n",
    "# gene_labels.npy : gene names (G,)  — bookkeeping\n",
    "# glmpca.npy      : A       (N, K)   — reconstruction TARGET for GASTON decoder\n",
    "\n",
    "glmpca_results = {}\n",
    "\n",
    "for slice_id, h5ad_path in slices_to_process.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Slice: {slice_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # --- Step 1: load raw data ---\n",
    "    print(\"[1/3] Loading data...\")\n",
    "    counts_mat, coords_mat, gene_labels = load_and_extract(h5ad_path, spot_umi_threshold)\n",
    "    print(f\"      counts_mat : {counts_mat.shape}  (N spots x G genes)\")\n",
    "    print(f\"      coords_mat : {coords_mat.shape}  (N spots x 2 coordinates)\")\n",
    "\n",
    "    # --- Step 2: fit GLM-PCA ---\n",
    "    # This is the slow step.  GLM-PCA fits the Poisson log-linear model by MLE,\n",
    "    # compressing 33k genes -> K=14 latent factors per spot.\n",
    "    print(\"\\n[2/3] Fitting GLM-PCA  (watch deviance decrease each iteration)...\")\n",
    "    A = run_glmpca(counts_mat, num_dims, penalty, num_iters, eps, num_genes)\n",
    "    print(f\"      Output A   : {A.shape}  (N spots x K={num_dims} factors)\")\n",
    "\n",
    "    # --- Step 3: save ---\n",
    "    print(\"\\n[3/3] Saving...\")\n",
    "    out_dir = f'{GLMPCA_OUTPUT_DIR}/{slice_id}'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    np.save(f'{out_dir}/counts_mat.npy',  counts_mat)\n",
    "    np.save(f'{out_dir}/coords_mat.npy',  coords_mat)\n",
    "    np.save(f'{out_dir}/gene_labels.npy', gene_labels)\n",
    "    np.save(f'{out_dir}/glmpca.npy',      A)\n",
    "    print(f\"      Saved to: {out_dir}/\")\n",
    "\n",
    "    glmpca_results[slice_id] = {\n",
    "        'counts_mat': counts_mat,\n",
    "        'coords_mat': coords_mat,\n",
    "        'gene_labels': gene_labels,\n",
    "        'A': A,\n",
    "    }\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All slices done.\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization: plot all K=14 latent factors spatially\n",
    "# =============================================================================\n",
    "# Each panel below shows one dimension z_k of the embedding, colored by value\n",
    "# and plotted at the (x, y) coordinates of each spot.\n",
    "#\n",
    "# What to look for:\n",
    "#   - Factors that show smooth spatial gradients aligned with cortical layers\n",
    "#     are the informative ones — they capture the expression variation that\n",
    "#     GASTON will learn to explain with a 1D isodepth coordinate.\n",
    "#   - Noisy or salt-and-pepper factors carry less signal.\n",
    "#\n",
    "# This is analogous to visualizing the principal components after PCA:\n",
    "# the first few components capture the dominant structure.\n",
    "\n",
    "def plot_glmpca_factors(A, coords_mat, slice_id, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot all K latent factors as spatial scatter plots.\n",
    "    Layout: 2 rows x (K/2) columns.\n",
    "    \"\"\"\n",
    "    K = A.shape[1]\n",
    "    ncols = K // 2\n",
    "    fig, axes = plt.subplots(2, ncols, figsize=(4 * ncols, 8))\n",
    "\n",
    "    for k in range(K):\n",
    "        r, c = divmod(k, ncols)\n",
    "        ax = axes[r, c]\n",
    "        sc = ax.scatter(\n",
    "            coords_mat[:, 0], coords_mat[:, 1],\n",
    "            c=A[:, k], cmap='RdBu_r', s=3, rasterized=True\n",
    "        )\n",
    "        ax.set_title(f'PC {k+1}', fontsize=9)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "        plt.colorbar(sc, ax=ax, shrink=0.6, pad=0.02)\n",
    "\n",
    "    plt.suptitle(\n",
    "        f'GLM-PCA latent factors — {slice_id}\\n'\n",
    "        f'(K={K} dims, model: X_ig ~ Poisson(exp(z_i^T w_g + mu_g)))',\n",
    "        fontsize=11, fontweight='bold'\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "for slice_id, data in glmpca_results.items():\n",
    "    print(f\"\\nPlotting factors for {slice_id}...\")\n",
    "    save_path = f'{GLMPCA_OUTPUT_DIR}/{slice_id}/glmpca_factors.png'\n",
    "    plot_glmpca_factors(data['A'], data['coords_mat'], slice_id, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Quick sanity check: variance explained per factor\n",
    "# =============================================================================\n",
    "# The factors from GLM-PCA are NOT orthogonal (unlike PCA), so there is no\n",
    "# single \"variance explained\" metric.  But we can check how much variance\n",
    "# each factor z_k has across spots — a factor with near-zero variance across\n",
    "# spots is essentially unused.\n",
    "\n",
    "print(\"Factor standard deviations across spots (higher = more active factor):\\n\")\n",
    "print(f\"{'Slice':<10}  \" + \"  \".join([f\"PC{k+1:02d}\" for k in range(num_dims)]))\n",
    "print(\"-\" * (10 + num_dims * 6))\n",
    "\n",
    "for slice_id, data in glmpca_results.items():\n",
    "    stds = data['A'].std(axis=0)\n",
    "    std_str = \"  \".join([f\"{s:.2f}\" for s in stds])\n",
    "    print(f\"{slice_id:<10}  {std_str}\")\n",
    "\n",
    "print()\n",
    "print(\"Note: factors are not orthogonal — std is only a rough activity measure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Summary\n",
    "# =============================================================================\n",
    "# This step reduced the gene expression data from G=33,538 dimensions\n",
    "# down to K=14 latent factors per spot, using a Poisson log-linear model\n",
    "# fitted by MLE.\n",
    "#\n",
    "# Pipeline role\n",
    "# -------------\n",
    "#   Raw counts X (N x 33538)\n",
    "#       |  GLM-PCA (this notebook)\n",
    "#       v\n",
    "#   Latent factors A (N x 14)    <-- GASTON decoder reconstruction target\n",
    "#   Spatial coords S (N x 2)     <-- GASTON encoder input\n",
    "#\n",
    "# The GASTON autoencoder (next step) will:\n",
    "#   encoder:  (x_i, y_i)  ->  scalar isodepth d_i\n",
    "#   decoder:  d_i          ->  A_hat_i in R^14\n",
    "#   loss:     ||A_i - A_hat_i||^2\n",
    "#\n",
    "# For C-GASTON (our project), A is also used to build the molecular\n",
    "# branch embedding: z_m = W_m * [d_i ; A_hat_i]  (dim = 1+14 = 15 -> 128)\n",
    "# which is then aligned with the vision branch via InfoNCE contrastive loss.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GLM-PCA complete.  Outputs saved:\")\n",
    "print(\"=\" * 60)\n",
    "for slice_id in slices_to_process:\n",
    "    out = f'{GLMPCA_OUTPUT_DIR}/{slice_id}'\n",
    "    print(f\"  {slice_id}/\")\n",
    "    print(f\"    counts_mat.npy  -> raw X  (N x G)\")\n",
    "    print(f\"    coords_mat.npy  -> (x,y)  (N x 2)\")\n",
    "    print(f\"    gene_labels.npy -> gene names\")\n",
    "    print(f\"    glmpca.npy      -> A      (N x {num_dims})  <-- next step input\")\n",
    "\n",
    "print()\n",
    "print(\"Next step: train GASTON neural network (3.Training_Gaston_NN/)\")\n",
    "\n",
    "print()\n",
    "print(\"Quick reload example:\")\n",
    "print(\"\"\"\n",
    "import numpy as np\n",
    "d = np.load  # shorthand\n",
    "sid = '151507'\n",
    "base = f'{GLMPCA_OUTPUT_DIR}/{sid}'\n",
    "A          = np.load(f'{base}/glmpca.npy')               # (N, 14)\n",
    "coords_mat = np.load(f'{base}/coords_mat.npy')           # (N, 2)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (gaston)",
   "language": "python",
   "name": "gaston"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}