{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md_intro",
   "metadata": {},
   "source": [
    "# Step 3: Training the GASTON Neural Network\n",
    "\n",
    "## What is this step doing?\n",
    "\n",
    "After Step 2, each spot $i$ has a **14-dimensional embedding** $\\mathbf{a}_i \\in \\mathbb{R}^{14}$ (from GLM-PCA) and a **2D spatial coordinate** $(x_i, y_i)$.  \n",
    "The question GASTON asks is:\n",
    "\n",
    "> *Can we explain most of the variation in $\\mathbf{a}_i$ using just a **single number** derived from the spot's spatial position?*\n",
    "\n",
    "That single number is called the **isodepth** $d_i$ — a learned 1D coordinate along the tissue's main axis of expression change (think of it as a \"depth into the cortex\" axis, but learned entirely from data).\n",
    "\n",
    "---\n",
    "\n",
    "## The model: an extreme-bottleneck autoencoder\n",
    "\n",
    "GASTON is a **bottleneck autoencoder** with bottleneck width = **1**:\n",
    "\n",
    "```\n",
    "  (x_i, y_i)                        a_i  ∈ R^14\n",
    "      │                               ▲\n",
    "      ▼                               │\n",
    "  Encoder φ_θ                    Decoder h_ψ\n",
    "  MLP [2 → 20 → 20 → 1]         MLP [1 → 20 → 20 → 14]\n",
    "      │                               │\n",
    "      └──────► d_i  ∈ R ─────────────┘\n",
    "                (isodepth)\n",
    "```\n",
    "\n",
    "**Encoder** $\\phi_\\theta : \\mathbb{R}^2 \\to \\mathbb{R}$  \n",
    "Maps each spot's (x, y) coordinates to a scalar isodepth. Architecture: 2 hidden layers of 20 ReLU units.\n",
    "\n",
    "**Decoder** $h_\\psi : \\mathbb{R} \\to \\mathbb{R}^{14}$  \n",
    "Reconstructs the GLM-PCA embedding from the scalar isodepth. Same architecture.\n",
    "\n",
    "**Loss** (MSE reconstruction):\n",
    "$$\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\| \\mathbf{a}_i - h_\\psi(\\phi_\\theta(x_i, y_i)) \\|_2^2$$\n",
    "\n",
    "This is a standard **autoencoder reconstruction loss** from EECS 545.\n",
    "\n",
    "---\n",
    "\n",
    "## Why width-1 bottleneck? What does it force the network to learn?\n",
    "\n",
    "A width-1 bottleneck forces the encoder to **rank all spots on a single axis** such that the decoder can reconstruct the 14-dim embedding from that rank alone.  \n",
    "Spots that share the same tissue layer will end up with similar $d_i$ values — not because we told the network about layers, but because their gene expression profiles $\\mathbf{a}_i$ are similar, and the only way the decoder can reconstruct them well is to assign them nearby isodepths.\n",
    "\n",
    "Geometrically: the decoder $h_\\psi$ is a **piecewise-linear curve** in $\\mathbb{R}^{14}$ (ReLU network from $\\mathbb{R}^1$), and the encoder is learning to project each spot onto the nearest point on that curve. This is analogous to **1D nonlinear PCA** (a.k.a. a principal curve).\n",
    "\n",
    "---\n",
    "\n",
    "## Design choices — other things worth trying\n",
    "\n",
    "This architecture is **not the only option**. Here are natural variants to consider:\n",
    "\n",
    "| What to change | Original | Alternative | Effect |\n",
    "|---|---|---|---|\n",
    "| **Bottleneck width** | 1 | 2 | 2D isodepth — captures more structure, harder to interpret |\n",
    "| **Network depth/width** | `[20, 20]` | `[64, 64, 64]` | More capacity, risk of overfitting on ~4k points |\n",
    "| **Loss function** | MSE on $\\mathbf{A}$ | Cosine similarity, or Poisson NLL | Different inductive bias on the embedding |\n",
    "| **Spatial regularization** | None | Add $\\lambda \\sum_{(i,j) \\text{ neighbors}} (d_i - d_j)^2$ | Encourages spatially smooth isodepth (like a graph Laplacian regularizer) |\n",
    "| **Input to encoder** | (x, y) only | (x, y, H&E patch features) | **This is exactly what C-GASTON adds** |\n",
    "| **Random restarts** | 30 | More/fewer | More restarts = better chance of escaping local minima |\n",
    "\n",
    "The last row is the key motivation for the C-GASTON project: the encoder currently ignores the H&E image, which contains rich morphological information about tissue structure.\n",
    "\n",
    "---\n",
    "\n",
    "## Why multiple random restarts?\n",
    "\n",
    "The MSE loss of a bottleneck autoencoder is **non-convex** — Adam can get stuck in different local minima depending on initialization. We run **30 independent training runs** with different random seeds and select the one with the lowest final loss. This is the same idea as running k-means with multiple initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 3: Training the GASTON Bottleneck Autoencoder\n",
    "# =============================================================================\n",
    "# EECS 545 Project: C-GASTON\n",
    "# Reference: https://gaston-tutorial.readthedocs.io/\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from gaston import neural_net\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "\n",
    "print(\"Libraries imported.\")\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"CUDA available  : {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU             : {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Paths\n",
    "# =============================================================================\n",
    "\n",
    "BASE_DIR = '/home/siruilf/A_new_dataset_for_gaston'\n",
    "\n",
    "# Input: GLM-PCA embeddings from Step 2\n",
    "GLMPCA_DIR = f'{BASE_DIR}/2.GLM_PC/glmpca_results'\n",
    "\n",
    "# Output: trained model checkpoints\n",
    "NN_OUTPUT_DIR = f'{BASE_DIR}/3.Training_Gaston__NN/nn_results'\n",
    "os.makedirs(NN_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"GLM-PCA input  : {GLMPCA_DIR}\")\n",
    "print(f\"NN output      : {NN_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Slices to train\n",
    "# =============================================================================\n",
    "\n",
    "slices_to_train = ['151507', '151508', '151673', '151674']\n",
    "\n",
    "print(\"Checking data availability...\")\n",
    "for sid in slices_to_train:\n",
    "    ok = (\n",
    "        os.path.exists(f'{GLMPCA_DIR}/{sid}/glmpca.npy') and\n",
    "        os.path.exists(f'{GLMPCA_DIR}/{sid}/coords_mat.npy')\n",
    "    )\n",
    "    print(f\"  {sid}: {'ready' if ok else 'MISSING — run Step 2 first'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Hyperparameters\n",
    "# =============================================================================\n",
    "#\n",
    "# isodepth_arch / expression_arch\n",
    "# --------------------------------\n",
    "# Both encoder and decoder are 2-hidden-layer MLPs with 20 units each and\n",
    "# ReLU activations.  The small size is intentional: ~4k training points\n",
    "# with a simple 1D bottleneck do not need a large network.  A larger network\n",
    "# would overfit and produce a non-smooth isodepth.\n",
    "#\n",
    "# epochs\n",
    "# ------\n",
    "# Full-batch gradient descent for 10,000 steps.  Because N ~4k fits easily\n",
    "# in GPU memory, there is no mini-batching — each step uses all spots.\n",
    "# (Mini-batch training is an option for larger datasets.)\n",
    "#\n",
    "# num_restarts\n",
    "# ------------\n",
    "# The loss landscape is non-convex (ReLU network, bottleneck).  We run 30\n",
    "# independent training runs with different random seeds and keep the model\n",
    "# with the lowest final reconstruction loss.  This is the same strategy as\n",
    "# multi-start k-means.\n",
    "#\n",
    "# optimizer\n",
    "# ---------\n",
    "# Adam with default learning rate (1e-3).  Standard choice for MLPs.\n",
    "\n",
    "isodepth_arch   = [20, 20]    # encoder: R^2 -> 20 -> 20 -> R^1\n",
    "expression_arch = [20, 20]    # decoder: R^1 -> 20 -> 20 -> R^14\n",
    "epochs          = 10000\n",
    "checkpoint      = 500         # save model state every 500 epochs\n",
    "optimizer       = 'adam'\n",
    "num_restarts    = 30\n",
    "device          = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Hyperparameters:\")\n",
    "print(f\"  encoder arch    : R^2 -> {isodepth_arch} -> R^1\")\n",
    "print(f\"  decoder arch    : R^1 -> {expression_arch} -> R^14\")\n",
    "print(f\"  epochs          : {epochs}\")\n",
    "print(f\"  checkpoint every: {checkpoint} epochs\")\n",
    "print(f\"  optimizer       : {optimizer}\")\n",
    "print(f\"  random restarts : {num_restarts}\")\n",
    "print(f\"  device          : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training function\n",
    "# =============================================================================\n",
    "\n",
    "def train_gaston(slice_id, glmpca_dir, output_dir,\n",
    "                 isodepth_arch, expression_arch,\n",
    "                 epochs, checkpoint, optimizer,\n",
    "                 num_restarts, device):\n",
    "    \"\"\"\n",
    "    Train the GASTON bottleneck autoencoder for one tissue slice.\n",
    "\n",
    "    Data flow\n",
    "    ---------\n",
    "    Load  A (N, 14) and S (N, 2)  from GLM-PCA step\n",
    "      |-- z-score normalize both  (neural_net.load_rescale_input_data)\n",
    "      |-- run `num_restarts` independent training runs\n",
    "      |-- each run: Adam for `epochs` steps, full-batch MSE loss\n",
    "      |-- save checkpoint every `checkpoint` steps + final model\n",
    "\n",
    "    The best model is selected in Step 4 (Process_NN_Output) by comparing\n",
    "    the final loss across all restarts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    slice_id       : str  e.g. '151507'\n",
    "    glmpca_dir     : str  directory containing glmpca.npy and coords_mat.npy\n",
    "    output_dir     : str  where to save model checkpoints\n",
    "    isodepth_arch  : list encoder hidden layer sizes\n",
    "    expression_arch: list decoder hidden layer sizes\n",
    "    epochs         : int  training steps per restart\n",
    "    checkpoint     : int  save frequency in epochs\n",
    "    optimizer      : str  'adam'\n",
    "    num_restarts   : int  number of independent random initializations\n",
    "    device         : str  'cuda' or 'cpu'\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Slice: {slice_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # --- Load GLM-PCA outputs ---\n",
    "    # A: the 14-dim embeddings — reconstruction TARGET for the decoder\n",
    "    # S: the (x, y) coordinates  — INPUT to the encoder\n",
    "    print(\"[1/3] Loading GLM-PCA data...\")\n",
    "    A = np.load(f'{glmpca_dir}/{slice_id}/glmpca.npy')\n",
    "    S = np.load(f'{glmpca_dir}/{slice_id}/coords_mat.npy')\n",
    "    print(f\"      A (embeddings) : {A.shape}  (N x K)\")\n",
    "    print(f\"      S (coordinates): {S.shape}  (N x 2)\")\n",
    "\n",
    "    # --- z-score normalize ---\n",
    "    # Normalize each feature to zero mean and unit variance.\n",
    "    # This is standard preprocessing before training an MLP — without it,\n",
    "    # features on very different scales cause unstable gradients.\n",
    "    # (Same as sklearn's StandardScaler, applied to both S and A.)\n",
    "    print(\"\\n[2/3] z-score normalizing inputs...\")\n",
    "    S_torch, A_torch = neural_net.load_rescale_input_data(S, A)\n",
    "    print(f\"      S_torch : {tuple(S_torch.shape)}  mean≈0, std≈1 per coordinate\")\n",
    "    print(f\"      A_torch : {tuple(A_torch.shape)}  mean≈0, std≈1 per dimension\")\n",
    "\n",
    "    # --- Train num_restarts independent models ---\n",
    "    # Each restart uses a fresh random weight initialization (different seed).\n",
    "    # The Adam optimizer runs for `epochs` full-batch gradient steps.\n",
    "    # We save a checkpoint every `checkpoint` epochs so we can inspect\n",
    "    # convergence or resume training if needed.\n",
    "    print(f\"\\n[3/3] Training {num_restarts} restarts x {epochs} epochs...\")\n",
    "    slice_out = f'{output_dir}/{slice_id}'\n",
    "    os.makedirs(slice_out, exist_ok=True)\n",
    "\n",
    "    for seed in range(num_restarts):\n",
    "        rep_dir = f'{slice_out}/rep{seed}'\n",
    "        os.makedirs(rep_dir, exist_ok=True)\n",
    "\n",
    "        mod, loss_list = neural_net.train(\n",
    "            S_torch, A_torch,\n",
    "            S_hidden_list=isodepth_arch,\n",
    "            A_hidden_list=expression_arch,\n",
    "            epochs=epochs,\n",
    "            checkpoint=checkpoint,\n",
    "            device=device,\n",
    "            save_dir=rep_dir,\n",
    "            optim=optimizer,\n",
    "            seed=seed,\n",
    "            save_final=True\n",
    "        )\n",
    "        print(f\"  rep{seed:02d}  final loss = {loss_list[-1]:.6f}\")\n",
    "\n",
    "    print(f\"\\nSlice {slice_id} done.  Results in: {slice_out}\")\n",
    "    return slice_out\n",
    "\n",
    "\n",
    "print(\"Function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Train all slices\n",
    "# =============================================================================\n",
    "# Each slice: 30 restarts x 10,000 epochs = 300,000 gradient steps.\n",
    "# Typical runtime: ~5-15 min per slice on a modern GPU.\n",
    "#\n",
    "# The best model per slice is NOT selected here — that happens in\n",
    "# Step 4 (Process_NN_Output), where we compare all 30 restarts by\n",
    "# their final reconstruction loss and optionally by ARI against ground truth.\n",
    "\n",
    "trained_paths = {}\n",
    "\n",
    "for slice_id in slices_to_train:\n",
    "    trained_paths[slice_id] = train_gaston(\n",
    "        slice_id       = slice_id,\n",
    "        glmpca_dir     = GLMPCA_DIR,\n",
    "        output_dir     = NN_OUTPUT_DIR,\n",
    "        isodepth_arch  = isodepth_arch,\n",
    "        expression_arch= expression_arch,\n",
    "        epochs         = epochs,\n",
    "        checkpoint     = checkpoint,\n",
    "        optimizer      = optimizer,\n",
    "        num_restarts   = num_restarts,\n",
    "        device         = device,\n",
    "    )\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All slices trained.\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Summary\n",
    "# =============================================================================\n",
    "# Each slice now has 30 independently trained bottleneck autoencoders.\n",
    "# The directory structure is:\n",
    "#\n",
    "#   nn_results/\n",
    "#   └── {slice_id}/\n",
    "#         ├── rep0/\n",
    "#         │    ├── Atorch.pt          <- normalized A (same for all reps)\n",
    "#         │    ├── Storch.pt          <- normalized S (same for all reps)\n",
    "#         │    ├── final_model.pt     <- trained model weights\n",
    "#         │    ├── min_loss.txt       <- final reconstruction loss\n",
    "#         │    └── model_epoch_*.pt   <- checkpoints every 500 epochs\n",
    "#         ├── rep1/\n",
    "#         ├── ...\n",
    "#         └── rep29/\n",
    "#\n",
    "# Next step (Step 4) loads all 30 models, selects the best by loss or ARI,\n",
    "# then runs the dp_related module to extract the isodepth and domain labels.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training complete.  File structure:\")\n",
    "print(\"=\" * 60)\n",
    "for sid in slices_to_train:\n",
    "    print(f\"  {NN_OUTPUT_DIR}/{sid}/\")\n",
    "    print(f\"    rep0/ .. rep{num_restarts-1}/\")\n",
    "    print(f\"      final_model.pt    <- autoencoder weights\")\n",
    "    print(f\"      min_loss.txt      <- final MSE loss (used to pick best rep)\")\n",
    "\n",
    "print()\n",
    "print(\"Next step: Process_NN_Output — pick best restart, extract isodepth & domains.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (gaston)",
   "language": "python",
   "name": "gaston"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
